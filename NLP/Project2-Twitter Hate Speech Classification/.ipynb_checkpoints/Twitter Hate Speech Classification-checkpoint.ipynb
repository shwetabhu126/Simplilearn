{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PGP AI - Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project 2: Help Twitter Combat Hate Speech Using NLP and Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Author:\n",
    ">\n",
    "> ***Saikat Narayan Bhattacharjya***\n",
    ">\n",
    ">  ***Email: <snbhattacharjya@gmail.com>***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DESCRIPTION\n",
    "\n",
    "Using NLP and ML, make a model to identify hate speech (racist or sexist tweets) in Twitter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Statement:  \n",
    "\n",
    "Twitter is the biggest platform where anybody and everybody can have their views heard. Some of these voices spread hate and negativity. Twitter is wary of its platform being used as a medium  to spread hate. \n",
    "\n",
    "You are a data scientist at Twitter, and you will help Twitter in identifying the tweets with hate speech and removing them from the platform. You will use NLP techniques, perform specific cleanup for tweets data, and make a robust model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Domain: Social Media"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis to be done: \n",
    "\n",
    "Clean up tweets and build a classification model by using NLP techniques, cleanup specific for tweets data, regularization and hyperparameter tuning using stratified k-fold and cross validation to get the best model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Content: \n",
    "\n",
    "1. id: identifier number of the tweet\n",
    "2. Label: 0 (non-hate) /1 (hate)\n",
    "3. Tweet: the text in the tweet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tasks: \n",
    "\n",
    "1. Load the tweets file using read_csv function from Pandas package.\n",
    "2. Get the tweets into a list for easy text cleanup and manipulation.\n",
    "3. To cleanup: \n",
    "    1. Normalize the casing.\n",
    "    2. Using regular expressions, remove user handles. These begin with '@’.\n",
    "    3. Using regular expressions, remove URLs.\n",
    "    4. Using TweetTokenizer from NLTK, tokenize the tweets into individual terms.\n",
    "    5. Remove stop words.\n",
    "    6. Remove redundant terms like ‘amp’, ‘rt’, etc.\n",
    "    7. Remove ‘#’ symbols from the tweet while retaining the term.\n",
    "4. Extra cleanup by removing terms with a length of 1.\n",
    "5. Check out the top terms in the tweets:\n",
    "    1. First, get all the tokenized terms into one large list.\n",
    "    2. Use the counter and find the 10 most common terms.\n",
    "6. Data formatting for predictive modeling:\n",
    "    1. Join the tokens back to form strings. This will be required for the vectorizers.\n",
    "    2. Assign x and y.\n",
    "    3. Perform train_test_split using sklearn.\n",
    "7. We’ll use TF-IDF values for the terms as a feature to get into a vector space model.\n",
    "    1. Import TF-IDF  vectorizer from sklearn.\n",
    "    2. Instantiate with a maximum of 5000 terms in your vocabulary.\n",
    "    3. Fit and apply on the train set.\n",
    "    4. Apply on the test set.\n",
    "8. Model building: Ordinary Logistic Regression\n",
    "    1. Instantiate Logistic Regression from sklearn with default parameters.\n",
    "    2. Fit into  the train data.\n",
    "    3. Make predictions for the train and the test set.\n",
    "9. Model evaluation: Accuracy, recall, and f_1 score.\n",
    "    1. Report the accuracy on the train set.\n",
    "    2. Report the recall on the train set: decent, high, or low.\n",
    "    3. Get the f1 score on the train set.\n",
    "10.\tLooks like you need to adjust the class imbalance, as the model seems to focus on the 0s.\n",
    "    1. Adjust the appropriate class in the LogisticRegression model.\n",
    "11.\tTrain again with the adjustment and evaluate.\n",
    "    1. Train the model on the train set.\n",
    "    2. Evaluate the predictions on the train set: accuracy, recall, and f_1 score.\n",
    "12.\tRegularization and Hyperparameter tuning:\n",
    "    1. Import GridSearch and StratifiedKFold because of class imbalance.\n",
    "    2. Provide the parameter grid to choose for ‘C’ and ‘penalty’ parameters.\n",
    "    3. Use a balanced class weight while instantiating the logistic regression.\n",
    "13.\tFind the parameters with the best recall in cross-validation.\n",
    "    1. Choose ‘recall’ as the metric for scoring.\n",
    "    2. Choose a stratified 4 fold cross-validation scheme.\n",
    "    3. Fit into  the train set.\n",
    "14.\tWhat are the best parameters?\n",
    "15.\tPredict and evaluate using the best estimator.\n",
    "    1. Use the best estimator from the grid search to make predictions on the test set.\n",
    "    2. What is the recall on the test set for the toxic comments?\n",
    "    3. What is the f_1 score?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting up the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from pprint import pprint\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_data = pd.read_csv('TwitterHate.csv')\n",
    "tweet_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting the tweet in a List sturcture for pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = tweet_data['tweet'].to_list()\n",
    "tweets[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining a Basic pre-processing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "def basic_tweet_cleanup(tweets):\n",
    "    #Lower casing\n",
    "    tweets = [tweet.lower() for tweet in tweets]\n",
    "    \n",
    "    #Removing @\n",
    "    tweets = [re.sub('@\\S+\\s+','',tweet) for tweet in tweets]\n",
    "    \n",
    "    #Removing URL \n",
    "    tweets = [re.sub('http\\S://\\S+','',tweet) for tweet in tweets]\n",
    "    \n",
    "    #Remove ‘#’ symbols from the tweet while retaining the term.\n",
    "    tweets = [re.sub('#','',tweet) for tweet in tweets]\n",
    "      \n",
    "    #Using TweetTokenizer from NLTK, tokenize the tweets into individual terms.\n",
    "    tweet_tokenizer = TweetTokenizer()\n",
    "    tweets = [tweet_tokenizer.tokenize(tweet) for tweet in tweets]\n",
    "    \n",
    "    #Remove stop words\n",
    "    tweets = [[token for token in tweet if token not in nltk.corpus.stopwords.words('english')] for tweet in tweets]\n",
    "    \n",
    "    #Remove redundant words like 'amp', 'rt'\n",
    "    tweets = [[token for token in tweet if token not in ['amp','rt']] for tweet in tweets]\n",
    "    \n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking the output after basic preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "tweets_cleaned = basic_tweet_cleanup(tweets)\n",
    "pprint(tweets_cleaned[:5], compact=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Original text data before pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(tweets[:5], compact=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining an Advanced Cleanup function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def advanced_tweet_cleanup(tweets):\n",
    "    #Filtering only alphabet words with length > 1\n",
    "    tweets_cleaned = [[token for token in tweet_tokens if token.isalpha() and len(token) > 1] for tweet_tokens in tweets]\n",
    "    return tweets_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking the output after Advanced cleanup of the text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_cleaned = advanced_tweet_cleanup(tweets_cleaned)\n",
    "pprint(tweets_cleaned[:5], compact=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Joining the tokens in a list to find the top ten common terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms = []\n",
    "\n",
    "for tweet in tweets_cleaned:\n",
    "    for token in tweet:\n",
    "        terms.append(token)\n",
    "\n",
    "print(\"Total Tokens: {}\".format(len(terms)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a table of top ten common words in the text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "counts_terms = Counter(terms)\n",
    "terms_df = pd.DataFrame(counts_terms.most_common(10), columns=['term', 'count'])\n",
    "terms_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualising the top ten common words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms_df.sort_values(by='count', ascending=True).plot(kind=\"barh\", x='term', figsize=(12,10), color='teal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding cleaned tweet data to the data frame for creating Bag of Words by TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_cleaned_sent = []\n",
    "\n",
    "for tweet in tweets_cleaned:\n",
    "    sent = \"\"\n",
    "    for token in tweet:\n",
    "        sent = sent + token + \" \"\n",
    "    tweets_cleaned_sent.append(sent[:-1])\n",
    "\n",
    "tweets_cleaned_sent[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_data['tweet_cleaned'] = tweets_cleaned_sent\n",
    "tweet_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assigning X and y for Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tweet_data['tweet_cleaned']\n",
    "y = tweet_data['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spliting the dataset into Train and Test set in the ratio of 70:30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state = 42)\n",
    "len(X_train), len(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialising the vectorizer with maximum features as 5000 (words/columns) for the creating the Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features = 5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the Train and Test feature matrix for prediction modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_bow = vectorizer.fit_transform(X_train)\n",
    "X_test_bow = vectorizer.fit_transform(X_test)\n",
    "\n",
    "X_train_bow.shape, X_test_bow.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a classification model using Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "log_reg = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg.fit(X_train_bow, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performing predictions on Train and Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = log_reg.predict(X_train_bow)\n",
    "y_test_pred = log_reg.predict(X_test_bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking the Accuracy and Performance Metrics without Regularisation and Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy Score for Training set: {}%\".format(accuracy_score(y_train, y_train_pred)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_train, y_train_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis of the result:\n",
    "\n",
    "It is understood that the dominance of class label '0' imbalances the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_data['label'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a classifier model by Logistic Regression using class weight as 'balanced'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression(class_weight='balanced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg.fit(X_train_bow, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = log_reg.predict(X_train_bow)\n",
    "y_test_pred = log_reg.predict(X_test_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy Score for Training set after Class Balanced: {}%\".format(accuracy_score(y_train, y_train_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_train_pred, y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For Regularisation and Hyperparameter Tuning, we are using GridsearchCV and StratifiedKFold "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a parameter grid of C and Penalty to find the best possible combination for a higher recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_params= {\n",
    "    \"C\": [0.01,0.1,1,10,100],\n",
    "    \"penalty\": [\"l1\",\"l2\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression(class_weight = \"balanced\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = GridSearchCV(estimator = log_reg, param_grid = search_params, cv = StratifiedKFold(4), scoring = \"recall\", \n",
    "                           n_jobs = -1, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "grid_search.fit(X_train_bow, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = grid_search.best_estimator_.predict(X_test_bow)\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
